{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d4fa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: imports & paths\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "from IPython.display import Video\n",
    "\n",
    "# Base paths\n",
    "BASE_DIR = \"34759_final_project_rect\"\n",
    "SEQ_DIR  = os.path.join(BASE_DIR, \"seq_03\")\n",
    "\n",
    "LEFT_DIR  = os.path.join(SEQ_DIR, \"image_02\", \"data\")  # LEFT\n",
    "RIGHT_DIR = os.path.join(SEQ_DIR, \"image_03\", \"data\")  # RIGHT\n",
    "\n",
    "CALIB_PATH = os.path.join(BASE_DIR, \"calib_cam_to_cam.txt\")\n",
    "\n",
    "print(\"Left images dir :\", LEFT_DIR)\n",
    "print(\"Right images dir:\", RIGHT_DIR)\n",
    "print(\"Calib file      :\", CALIB_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156a06cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: load calibration (P_rect_02, P_rect_03) and derive intrinsics + baseline\n",
    "\n",
    "def parse_kitti_calib_cam_to_cam(calib_path):\n",
    "    \"\"\"\n",
    "    Load calib_cam_to_cam.txt and return a dict mapping key -> numpy array.\n",
    "    For keys with 12 or 9 numbers, reshape to (3,4) or (3,3) respectively.\n",
    "    \"\"\"\n",
    "    params = {}\n",
    "    with open(calib_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if len(line) == 0 or line.startswith(\"#\"):\n",
    "                continue\n",
    "            if \":\" not in line:\n",
    "                continue\n",
    "            key, value = line.split(\":\", 1)\n",
    "            key = key.strip()\n",
    "            value = value.strip()\n",
    "            if len(value) == 0:\n",
    "                continue\n",
    "            parts = [float(x) for x in value.split()]\n",
    "            if len(parts) == 12:\n",
    "                params[key] = np.array(parts).reshape(3, 4)\n",
    "            elif len(parts) == 9:\n",
    "                params[key] = np.array(parts).reshape(3, 3)\n",
    "            elif len(parts) == 2:\n",
    "                params[key] = np.array(parts)\n",
    "            else:\n",
    "                params[key] = np.array(parts)\n",
    "    return params\n",
    "\n",
    "calib = parse_kitti_calib_cam_to_cam(CALIB_PATH)\n",
    "\n",
    "P2 = calib[\"P_rect_02\"]  # Left camera projection matrix (3x4)\n",
    "P3 = calib[\"P_rect_03\"]  # Right camera projection matrix (3x4)\n",
    "\n",
    "print(\"P_rect_02:\\n\", P2)\n",
    "print(\"\\nP_rect_03:\\n\", P3)\n",
    "\n",
    "# Intrinsics from left camera\n",
    "fx = P2[0, 0]\n",
    "fy = P2[1, 1]\n",
    "cx = P2[0, 2]\n",
    "cy = P2[1, 2]\n",
    "\n",
    "# Baseline: for rectified pair, P3[0,3] = -fx * B (left Tx assumed 0)\n",
    "baseline = -P3[0, 3] / fx\n",
    "\n",
    "print(f\"\\nfx={fx:.3f}, fy={fy:.3f}, cx={cx:.3f}, cy={cy:.3f}, baseline={baseline:.4f} m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c7f7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: list images and helper to load frames\n",
    "\n",
    "def sorted_image_paths(img_dir):\n",
    "    files = [f for f in os.listdir(img_dir) if f.lower().endswith(\".png\")]\n",
    "    files = sorted(files)\n",
    "    return [os.path.join(img_dir, f) for f in files]\n",
    "\n",
    "left_paths  = sorted_image_paths(LEFT_DIR)\n",
    "right_paths = sorted_image_paths(RIGHT_DIR)\n",
    "\n",
    "num_frames = min(len(left_paths), len(right_paths))\n",
    "print(f\"Found {num_frames} frames in seq_03\")\n",
    "\n",
    "def load_frame(frame_idx):\n",
    "    if frame_idx < 0 or frame_idx >= num_frames:\n",
    "        raise IndexError(f\"Frame {frame_idx} out of range [0, {num_frames-1}]\")\n",
    "    left  = cv2.imread(left_paths[frame_idx])\n",
    "    right = cv2.imread(right_paths[frame_idx])\n",
    "    return left, right\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b1816f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: create stereo matcher for disparity\n",
    "\n",
    "# Parameters can be tuned\n",
    "window_size = 5\n",
    "min_disp = 0\n",
    "num_disp = 128  # must be divisible by 16\n",
    "\n",
    "stereo = cv2.StereoSGBM_create(\n",
    "    minDisparity=min_disp,\n",
    "    numDisparities=num_disp,\n",
    "    blockSize=7,\n",
    "    P1=8 * 3 * window_size**2,\n",
    "    P2=32 * 3 * window_size**2,\n",
    "    disp12MaxDiff=1,\n",
    "    uniquenessRatio=10,\n",
    "    speckleWindowSize=100,\n",
    "    speckleRange=32\n",
    ")\n",
    "\n",
    "def compute_disparity(left_bgr, right_bgr):\n",
    "    left_gray  = cv2.cvtColor(left_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    right_gray = cv2.cvtColor(right_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    disp = stereo.compute(left_gray, right_gray).astype(np.float32) / 16.0\n",
    "    # Invalid disparities are usually <= 0\n",
    "    return disp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b754216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: load YOLO detector (pretrained on COCO)\n",
    "\n",
    "# You can switch to a bigger model if GPU is available, e.g. \"yolov8m.pt\"\n",
    "yolo_model = YOLO(\"yolov8m.pt\")\n",
    "\n",
    "# Map COCO classes to your desired classes\n",
    "COCO_TO_OUR = {\n",
    "    \"person\": \"Pedestrian\",\n",
    "    \"bicycle\": \"Bike\",\n",
    "    \"car\": \"Car\",\n",
    "}\n",
    "\n",
    "def detect_objects(left_bgr, conf_thresh=0.25):\n",
    "    \"\"\"\n",
    "    Run YOLO on left image, return list of detections:\n",
    "    each detection is dict with keys:\n",
    "    'type', 'bbox', 'score'\n",
    "    bbox = [x1, y1, x2, y2] in pixel coordinates.\n",
    "    \"\"\"\n",
    "    results = yolo_model(left_bgr, imgsz=960, verbose=False)[0]\n",
    "\n",
    "    detections = []\n",
    "    for box in results.boxes:\n",
    "        cls_id = int(box.cls[0].item())\n",
    "        conf   = float(box.conf[0].item())\n",
    "        if conf < conf_thresh:\n",
    "            continue\n",
    "        x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
    "        coco_name = results.names[cls_id]\n",
    "        if coco_name not in COCO_TO_OUR:\n",
    "            continue\n",
    "\n",
    "        our_type = COCO_TO_OUR[coco_name]\n",
    "        detections.append({\n",
    "            \"type\": our_type,\n",
    "            \"bbox\": [x1, y1, x2, y2],\n",
    "            \"score\": conf\n",
    "        })\n",
    "    return detections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5a4040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: 3D estimation helpers\n",
    "\n",
    "def estimate_box_3d(bbox, disparity_map, fx, fy, cx, cy, baseline, min_valid_pixels=50):\n",
    "    \"\"\"\n",
    "    Given bbox [x1, y1, x2, y2] and disparity map, estimate:\n",
    "    X, Y, Z (m) and width, height (m) of object in camera coordinates.\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    x1_i, y1_i = int(max(0, x1)), int(max(0, y1))\n",
    "    x2_i, y2_i = int(min(disparity_map.shape[1]-1, x2)), int(min(disparity_map.shape[0]-1, y2))\n",
    "\n",
    "    if x2_i <= x1_i or y2_i <= y1_i:\n",
    "        return None  # invalid box\n",
    "\n",
    "    disp_crop = disparity_map[y1_i:y2_i, x1_i:x2_i]\n",
    "    valid_mask = disp_crop > 0.5  # threshold to filter garbage disparities\n",
    "\n",
    "    if valid_mask.sum() < min_valid_pixels:\n",
    "        return None  # too few valid disparity pixels\n",
    "\n",
    "    disp_valid = disp_crop[valid_mask]\n",
    "    median_disp = np.median(disp_valid)\n",
    "\n",
    "    if median_disp <= 0:\n",
    "        return None\n",
    "\n",
    "    # Depth\n",
    "    Z = fx * baseline / median_disp  # meters\n",
    "\n",
    "    # Box center in pixels\n",
    "    u = 0.5 * (x1 + x2)\n",
    "    v = 0.5 * (y1 + y2)\n",
    "\n",
    "    # Back-project to 3D\n",
    "    X = (u - cx) * Z / fx\n",
    "    Y = (v - cy) * Z / fy\n",
    "\n",
    "    # Approximate dimensions from pixel size at that depth\n",
    "    w_px = x2 - x1\n",
    "    h_px = y2 - y1\n",
    "\n",
    "    width_m  = w_px * Z / fx\n",
    "    height_m = h_px * Z / fy\n",
    "\n",
    "    return {\n",
    "        \"X\": float(X),\n",
    "        \"Y\": float(Y),\n",
    "        \"Z\": float(Z),\n",
    "        \"width_m\": float(width_m),\n",
    "        \"height_m\": float(height_m),\n",
    "        \"median_disp\": float(median_disp),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3862d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: drawing function\n",
    "\n",
    "def draw_detections(left_bgr, detections_with_3d):\n",
    "    \"\"\"\n",
    "    detections_with_3d: list of dicts with keys:\n",
    "      'type', 'bbox', 'score', 'X','Y','Z','width_m','height_m' (some may be None if 3D failed)\n",
    "    \"\"\"\n",
    "    img = left_bgr.copy()\n",
    "\n",
    "    for det in detections_with_3d:\n",
    "        x1, y1, x2, y2 = det[\"bbox\"]\n",
    "        obj_type = det[\"type\"]\n",
    "        score    = det[\"score\"]\n",
    "        pt1 = (int(x1), int(y1))\n",
    "        pt2 = (int(x2), int(y2))\n",
    "\n",
    "        color = (0, 255, 0)  # you can vary by class if you like\n",
    "        cv2.rectangle(img, pt1, pt2, color, 2)\n",
    "\n",
    "        # Prepare text lines\n",
    "        line1 = f\"{obj_type} ({score:.2f})\"\n",
    "        if det.get(\"Z\") is not None:\n",
    "            X, Y, Z = det[\"X\"], det[\"Y\"], det[\"Z\"]\n",
    "            Wm, Hm  = det[\"width_m\"], det[\"height_m\"]\n",
    "            line2 = f\"Z={Z:.1f}m, X={X:.1f}, Y={Y:.1f}\"\n",
    "            line3 = f\"H={Hm:.1f}m, W={Wm:.1f}m\"\n",
    "        else:\n",
    "            line2 = \"3D: n/a\"\n",
    "            line3 = \"\"\n",
    "\n",
    "        # Place text above/inside box\n",
    "        y0 = max(int(y1) - 30, 0)\n",
    "        cv2.putText(img, line1, (int(x1), y0),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2, cv2.LINE_AA)\n",
    "\n",
    "        y1_txt = max(int(y1) - 12, 0)\n",
    "        cv2.putText(img, line2, (int(x1), y1_txt),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2, cv2.LINE_AA)\n",
    "\n",
    "        if line3:\n",
    "            y2_txt = min(int(y2) + 15, img.shape[0]-5)\n",
    "            cv2.putText(img, line3, (int(x1), y2_txt),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2, cv2.LINE_AA)\n",
    "\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f53ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: sample a few frames and visualize detections + 3D info\n",
    "\n",
    "sample_indices = [0, num_frames // 4, num_frames // 2]  # pick 3 frames across sequence\n",
    "sample_indices = [idx for idx in sample_indices if 0 <= idx < num_frames]\n",
    "\n",
    "plt.figure(figsize=(15, 5 * len(sample_indices)))\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    left, right = load_frame(idx)\n",
    "    disparity = compute_disparity(left, right)\n",
    "\n",
    "    # Detect objects\n",
    "    dets = detect_objects(left, conf_thresh=0.4)\n",
    "\n",
    "    dets_with_3d = []\n",
    "    for det in dets:\n",
    "        info_3d = estimate_box_3d(det[\"bbox\"], disparity, fx, fy, cx, cy, baseline)\n",
    "        if info_3d is not None:\n",
    "            det_with_3d = {**det, **info_3d}\n",
    "        else:\n",
    "            det_with_3d = {**det, \"X\": None, \"Y\": None, \"Z\": None,\n",
    "                           \"width_m\": None, \"height_m\": None}\n",
    "        dets_with_3d.append(det_with_3d)\n",
    "\n",
    "    img_annotated = draw_detections(left, dets_with_3d)\n",
    "    img_rgb = cv2.cvtColor(img_annotated, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    plt.subplot(len(sample_indices), 1, i + 1)\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.title(f\"Frame {idx}: {len(dets_with_3d)} detections\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79d9515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: process entire sequence and save annotated video\n",
    "\n",
    "output_video_path = os.path.join(\"seq_03_annotated_yolo_3d.mp4\")\n",
    "fps = 10  # adjust to taste\n",
    "\n",
    "# Get frame size from first frame\n",
    "first_left, _ = load_frame(0)\n",
    "h, w = first_left.shape[:2]\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "writer = cv2.VideoWriter(output_video_path, fourcc, fps, (w, h))\n",
    "\n",
    "for idx in tqdm(range(num_frames), desc=\"Processing seq_03\"):\n",
    "    left, right = load_frame(idx)\n",
    "    disparity = compute_disparity(left, right)\n",
    "\n",
    "    dets = detect_objects(left, conf_thresh=0.4)\n",
    "\n",
    "    dets_with_3d = []\n",
    "    for det in dets:\n",
    "        info_3d = estimate_box_3d(det[\"bbox\"], disparity, fx, fy, cx, cy, baseline)\n",
    "        if info_3d is not None:\n",
    "            det_with_3d = {**det, **info_3d}\n",
    "        else:\n",
    "            det_with_3d = {**det, \"X\": None, \"Y\": None, \"Z\": None,\n",
    "                           \"width_m\": None, \"height_m\": None}\n",
    "        dets_with_3d.append(det_with_3d)\n",
    "\n",
    "    frame_annotated = draw_detections(left, dets_with_3d)\n",
    "    writer.write(frame_annotated)\n",
    "\n",
    "writer.release()\n",
    "print(\"Saved annotated video to:\", output_video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1916df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: 3D Kalman filter and Track class\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "class KalmanFilter3D:\n",
    "    def __init__(self, dt=0.1, process_var=1.0, meas_var=1.0):\n",
    "        \"\"\"\n",
    "        Simple constant-velocity Kalman filter in 3D.\n",
    "        State: [X, Y, Z, Vx, Vy, Vz]^T\n",
    "        Measurement: [X, Y, Z]\n",
    "        \"\"\"\n",
    "        self.dt = dt\n",
    "\n",
    "        # State transition\n",
    "        self.F = np.eye(6)\n",
    "        self.F[0, 3] = dt\n",
    "        self.F[1, 4] = dt\n",
    "        self.F[2, 5] = dt\n",
    "\n",
    "        # Measurement matrix\n",
    "        self.H = np.zeros((3, 6))\n",
    "        self.H[0, 0] = 1.0\n",
    "        self.H[1, 1] = 1.0\n",
    "        self.H[2, 2] = 1.0\n",
    "\n",
    "        # Process and measurement noise\n",
    "        self.Q = process_var * np.eye(6)\n",
    "        self.R = meas_var * np.eye(3)\n",
    "\n",
    "    def init_state(self, X, Y, Z):\n",
    "        x = np.zeros((6, 1))\n",
    "        x[0, 0] = X\n",
    "        x[1, 0] = Y\n",
    "        x[2, 0] = Z\n",
    "        P = np.eye(6) * 10.0  # large initial uncertainty\n",
    "        return x, P\n",
    "\n",
    "    def predict(self, x, P):\n",
    "        x = self.F @ x\n",
    "        P = self.F @ P @ self.F.T + self.Q\n",
    "        return x, P\n",
    "\n",
    "    def update(self, x, P, meas):\n",
    "        z = np.array(meas, dtype=float).reshape(3, 1)\n",
    "        y = z - self.H @ x                       # innovation\n",
    "        S = self.H @ P @ self.H.T + self.R       # innovation covariance\n",
    "        K = P @ self.H.T @ np.linalg.inv(S)      # Kalman gain\n",
    "        x = x + K @ y\n",
    "        P = (np.eye(6) - K @ self.H) @ P\n",
    "        return x, P\n",
    "\n",
    "\n",
    "class Track:\n",
    "    _next_id = 0\n",
    "\n",
    "    def __init__(self, det, kf: KalmanFilter3D):\n",
    "        \"\"\"\n",
    "        det: dict with keys 'X','Y','Z','bbox','type','score'\n",
    "        kf : shared KalmanFilter3D instance\n",
    "        \"\"\"\n",
    "        self.id = Track._next_id\n",
    "        Track._next_id += 1\n",
    "\n",
    "        self.kf = kf\n",
    "        X, Y, Z = det[\"X\"], det[\"Y\"], det[\"Z\"]\n",
    "        self.x, self.P = self.kf.init_state(X, Y, Z)\n",
    "\n",
    "        self.bbox = det[\"bbox\"]\n",
    "        self.type = det[\"type\"]\n",
    "        self.score = det[\"score\"]\n",
    "\n",
    "        # Store box size in pixels for reprojection\n",
    "        x1, y1, x2, y2 = self.bbox\n",
    "        self.width_px  = float(x2 - x1)\n",
    "        self.height_px = float(y2 - y1)\n",
    "\n",
    "        self.age = 1\n",
    "        self.time_since_update = 0\n",
    "        self.hits = 1\n",
    "\n",
    "    def predict(self):\n",
    "        # Pure motion prediction in 3D\n",
    "        self.x, self.P = self.kf.predict(self.x, self.P)\n",
    "        self.age += 1\n",
    "        self.time_since_update += 1\n",
    "        # We do NOT update bbox here anymore; we will reproject from 3D state when drawing\n",
    "\n",
    "    def update(self, det):\n",
    "        X, Y, Z = det[\"X\"], det[\"Y\"], det[\"Z\"]\n",
    "        self.x, self.P = self.kf.update(self.x, self.P, [X, Y, Z])\n",
    "        self.bbox = det[\"bbox\"]\n",
    "        self.type = det[\"type\"]\n",
    "        self.score = det[\"score\"]\n",
    "\n",
    "        # Update box size in pixels from measurement\n",
    "        x1, y1, x2, y2 = self.bbox\n",
    "        self.width_px  = float(x2 - x1)\n",
    "        self.height_px = float(y2 - y1)\n",
    "\n",
    "        self.time_since_update = 0\n",
    "        self.hits += 1\n",
    "\n",
    "    def get_state(self):\n",
    "        X = float(self.x[0, 0])\n",
    "        Y = float(self.x[1, 0])\n",
    "        Z = float(self.x[2, 0])\n",
    "        return X, Y, Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b447270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: associate detections with existing tracks using 3D distance\n",
    "\n",
    "def associate_detections_to_tracks(tracks, detections, max_dist=5.0):\n",
    "    \"\"\"\n",
    "    tracks:    list[Track]\n",
    "    detections: list[det dict with 'X','Y','Z']\n",
    "    Returns:\n",
    "        matches: list of (track_idx, det_idx)\n",
    "        unmatched_tracks: list of track indices\n",
    "        unmatched_detections: list of detection indices\n",
    "    \"\"\"\n",
    "    if len(tracks) == 0:\n",
    "        return [], [], list(range(len(detections)))\n",
    "    if len(detections) == 0:\n",
    "        return [], list(range(len(tracks))), []\n",
    "\n",
    "    cost = np.zeros((len(tracks), len(detections)), dtype=float)\n",
    "\n",
    "    for ti, tr in enumerate(tracks):\n",
    "        X_t, Y_t, Z_t = tr.get_state()\n",
    "        for di, det in enumerate(detections):\n",
    "            X_d, Y_d, Z_d = det[\"X\"], det[\"Y\"], det[\"Z\"]\n",
    "            if X_d is None or Y_d is None or Z_d is None:\n",
    "                cost[ti, di] = 1e6  # basically \"infinite\"\n",
    "            else:\n",
    "                dist = np.linalg.norm([X_t - X_d, Y_t - Y_d, Z_t - Z_d])\n",
    "                cost[ti, di] = dist\n",
    "\n",
    "    row_ind, col_ind = linear_sum_assignment(cost)\n",
    "\n",
    "    matches = []\n",
    "    unmatched_tracks = list(range(len(tracks)))\n",
    "    unmatched_detections = list(range(len(detections)))\n",
    "\n",
    "    for r, c in zip(row_ind, col_ind):\n",
    "        if cost[r, c] > max_dist:\n",
    "            continue  # too far, don't match\n",
    "        matches.append((r, c))\n",
    "        if r in unmatched_tracks:\n",
    "            unmatched_tracks.remove(r)\n",
    "        if c in unmatched_detections:\n",
    "            unmatched_detections.remove(c)\n",
    "\n",
    "    return matches, unmatched_tracks, unmatched_detections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7859b901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: draw tracks (with IDs, 3D positions, and occlusion handling)\n",
    "\n",
    "def draw_tracks(left_bgr, tracks, fx, fy, cx, cy):\n",
    "    \"\"\"\n",
    "    Draws all active tracks on the image.\n",
    "    For tracks with time_since_update > 0, we draw predicted-only positions (yellow).\n",
    "    For updated tracks, we draw in green.\n",
    "    \"\"\"\n",
    "    img = left_bgr.copy()\n",
    "    H, W = img.shape[:2]\n",
    "\n",
    "    for tr in tracks:\n",
    "        X, Y, Z = tr.get_state()\n",
    "\n",
    "        # If depth goes crazy, skip drawing\n",
    "        if Z <= 0.5 or Z > 200.0:\n",
    "            continue\n",
    "\n",
    "        # Project 3D center -> image coordinates\n",
    "        u = fx * X / Z + cx\n",
    "        v = fy * Y / Z + cy\n",
    "\n",
    "        # Use stored box size (in pixels) to build bbox around (u, v)\n",
    "        w = tr.width_px\n",
    "        h = tr.height_px\n",
    "\n",
    "        x1 = int(u - w / 2)\n",
    "        y1 = int(v - h / 2)\n",
    "        x2 = int(u + w / 2)\n",
    "        y2 = int(v + h / 2)\n",
    "\n",
    "        # Clamp to image bounds\n",
    "        x1 = max(0, min(W - 1, x1))\n",
    "        x2 = max(0, min(W - 1, x2))\n",
    "        y1 = max(0, min(H - 1, y1))\n",
    "        y2 = max(0, min(H - 1, y2))\n",
    "\n",
    "        # Color: green if updated this frame, yellow if only predicted\n",
    "        if tr.time_since_update == 0:\n",
    "            color = (0, 255, 0)   # updated\n",
    "            status = \"\"\n",
    "        else:\n",
    "            color = (0, 255, 255) # predicted\n",
    "            status = \" (pred)\"\n",
    "\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
    "\n",
    "        line1 = f\"ID {tr.id} {tr.type}{status}\"\n",
    "        line2 = f\"Z={Z:.1f}m, X={X:.1f}, Y={Y:.1f}\"\n",
    "\n",
    "        y0 = max(y1 - 25, 0)\n",
    "        y1_txt = max(y1 - 7, 0)\n",
    "\n",
    "        cv2.putText(img, line1, (x1, y0),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2, cv2.LINE_AA)\n",
    "        cv2.putText(img, line2, (x1, y1_txt),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2, cv2.LINE_AA)\n",
    "\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb213d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: run tracker on a few consecutive frames for visualization\n",
    "\n",
    "# Make sure fps exists; if not, define it (used for dt)\n",
    "try:\n",
    "    fps\n",
    "except NameError:\n",
    "    fps = 10.0\n",
    "\n",
    "tracks = []\n",
    "Track._next_id = 0\n",
    "\n",
    "kf_3d = KalmanFilter3D(dt=1.0 / fps, process_var=5.0, meas_var=1.0)\n",
    "max_age = 50        # frames to keep \"dead\" tracks alive (3s if fps=10)\n",
    "max_dist = 40.0     # max 3D distance in meters for association\n",
    "min_3d_conf_pixels = 50  # you can keep this, or slightly lower if needed\n",
    "\n",
    "start_idx = 0\n",
    "end_idx = min(start_idx + 20, num_frames)  # small window\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 4 * (end_idx - start_idx)//5 + 5))\n",
    "\n",
    "plot_row = 0\n",
    "\n",
    "for frame_idx in range(start_idx, end_idx):\n",
    "    left, right = load_frame(frame_idx)\n",
    "    disparity = compute_disparity(left, right)\n",
    "\n",
    "    # Step 1: detect objects in this frame\n",
    "    dets = detect_objects(left, conf_thresh=0.4)\n",
    "\n",
    "    # Step 2: get 3D for each detection (skip those without valid 3D)\n",
    "    dets3d = []\n",
    "    for det in dets:\n",
    "        info_3d = estimate_box_3d(det[\"bbox\"], disparity, fx, fy, cx, cy, baseline,\n",
    "                                  min_valid_pixels=min_3d_conf_pixels)\n",
    "        if info_3d is not None:\n",
    "            dets3d.append({**det, **info_3d})\n",
    "\n",
    "    # Step 3: predict all existing tracks\n",
    "    for tr in tracks:\n",
    "        tr.predict()\n",
    "\n",
    "    # Step 4: associate detections to tracks\n",
    "    matches, unmatched_tracks, unmatched_dets = associate_detections_to_tracks(\n",
    "        tracks, dets3d, max_dist=max_dist\n",
    "    )\n",
    "\n",
    "    # Step 5: update matched tracks\n",
    "    for t_idx, d_idx in matches:\n",
    "        tracks[t_idx].update(dets3d[d_idx])\n",
    "\n",
    "    # Step 6: create new tracks for unmatched detections\n",
    "    for d_idx in unmatched_dets:\n",
    "        tr = Track(dets3d[d_idx], kf_3d)\n",
    "        tracks.append(tr)\n",
    "\n",
    "    # Step 7: remove dead tracks\n",
    "    new_tracks = []\n",
    "    for tr in tracks:\n",
    "        if tr.time_since_update <= max_age:\n",
    "            new_tracks.append(tr)\n",
    "    tracks = new_tracks\n",
    "\n",
    "    # Visualize every 5th frame\n",
    "    if frame_idx % 5 == 0:\n",
    "        plot_row += 1\n",
    "        img_tr = draw_tracks(left, tracks, fx, fy, cx, cy)\n",
    "        img_tr_rgb = cv2.cvtColor(img_tr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        plt.subplot((end_idx - start_idx) // 5 + 1, 1, plot_row)\n",
    "        plt.imshow(img_tr_rgb)\n",
    "        plt.title(f\"Frame {frame_idx} with {len(tracks)} active tracks\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2355debf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: full seq_03 tracking video with IDs and occlusion prediction\n",
    "\n",
    "output_video_path_tracks = os.path.join(\"seq_03_tracking_3d.mp4\")\n",
    "fps = 10  # same as before, or adjust\n",
    "\n",
    "first_left, _ = load_frame(0)\n",
    "h, w = first_left.shape[:2]\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "writer = cv2.VideoWriter(output_video_path_tracks, fourcc, fps, (w, h))\n",
    "\n",
    "# Re-init tracker state for full run\n",
    "kf_3d = KalmanFilter3D(dt=1.0 / fps, process_var=5.0, meas_var=1.0)\n",
    "max_age = 50        # frames to keep \"dead\" tracks alive (3s if fps=10)\n",
    "max_dist = 40.0     # max 3D distance in meters for association\n",
    "min_3d_conf_pixels = 50  # you can keep this, or slightly lower if needed\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for frame_idx in tqdm(range(num_frames), desc=\"Tracking seq_03\"):\n",
    "    left, right = load_frame(frame_idx)\n",
    "    disparity = compute_disparity(left, right)\n",
    "\n",
    "    dets = detect_objects(left, conf_thresh=0.4)\n",
    "\n",
    "    dets3d = []\n",
    "    for det in dets:\n",
    "        info_3d = estimate_box_3d(det[\"bbox\"], disparity, fx, fy, cx, cy, baseline,\n",
    "                                  min_valid_pixels=min_3d_conf_pixels)\n",
    "        if info_3d is not None:\n",
    "            dets3d.append({**det, **info_3d})\n",
    "\n",
    "    # Predict step\n",
    "    for tr in tracks:\n",
    "        tr.predict()\n",
    "\n",
    "    # Associate\n",
    "    matches, unmatched_tracks, unmatched_dets = associate_detections_to_tracks(\n",
    "        tracks, dets3d, max_dist=max_dist\n",
    "    )\n",
    "\n",
    "    # Update matched\n",
    "    for t_idx, d_idx in matches:\n",
    "        tracks[t_idx].update(dets3d[d_idx])\n",
    "\n",
    "    # New tracks\n",
    "    for d_idx in unmatched_dets:\n",
    "        tracks.append(Track(dets3d[d_idx], kf_3d))\n",
    "\n",
    "    # Remove stale\n",
    "    new_tracks = []\n",
    "    for tr in tracks:\n",
    "        if tr.time_since_update <= max_age:\n",
    "            new_tracks.append(tr)\n",
    "    tracks = new_tracks\n",
    "\n",
    "    # Draw\n",
    "    frame_annotated = draw_tracks(left, tracks, fx, fy, cx, cy)\n",
    "    writer.write(frame_annotated)\n",
    "\n",
    "writer.release()\n",
    "print(\"Saved tracking video to:\", output_video_path_tracks)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
