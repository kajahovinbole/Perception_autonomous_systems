{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d4fa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: imports & paths\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "from IPython.display import Video\n",
    "\n",
    "# Base paths\n",
    "BASE_DIR = \"34759_final_project_rect\"\n",
    "SEQ_DIR  = os.path.join(BASE_DIR, \"seq_03\")\n",
    "\n",
    "LEFT_DIR  = os.path.join(SEQ_DIR, \"image_02\", \"data\")  # LEFT\n",
    "RIGHT_DIR = os.path.join(SEQ_DIR, \"image_03\", \"data\")  # RIGHT\n",
    "\n",
    "CALIB_PATH = os.path.join(BASE_DIR, \"calib_cam_to_cam.txt\")\n",
    "\n",
    "print(\"Left images dir :\", LEFT_DIR)\n",
    "print(\"Right images dir:\", RIGHT_DIR)\n",
    "print(\"Calib file      :\", CALIB_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156a06cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: load calibration (P_rect_02, P_rect_03) and derive intrinsics + baseline\n",
    "\n",
    "def parse_kitti_calib_cam_to_cam(calib_path):\n",
    "    \"\"\"\n",
    "    Load calib_cam_to_cam.txt and return a dict mapping key -> numpy array.\n",
    "    For keys with 12 or 9 numbers, reshape to (3,4) or (3,3) respectively.\n",
    "    \"\"\"\n",
    "    params = {}\n",
    "    with open(calib_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if len(line) == 0 or line.startswith(\"#\"):\n",
    "                continue\n",
    "            if \":\" not in line:\n",
    "                continue\n",
    "            key, value = line.split(\":\", 1)\n",
    "            key = key.strip()\n",
    "            value = value.strip()\n",
    "            if len(value) == 0:\n",
    "                continue\n",
    "            parts = [float(x) for x in value.split()]\n",
    "            if len(parts) == 12:\n",
    "                params[key] = np.array(parts).reshape(3, 4)\n",
    "            elif len(parts) == 9:\n",
    "                params[key] = np.array(parts).reshape(3, 3)\n",
    "            elif len(parts) == 2:\n",
    "                params[key] = np.array(parts)\n",
    "            else:\n",
    "                params[key] = np.array(parts)\n",
    "    return params\n",
    "\n",
    "calib = parse_kitti_calib_cam_to_cam(CALIB_PATH)\n",
    "\n",
    "P2 = calib[\"P_rect_02\"]  # Left camera projection matrix (3x4)\n",
    "P3 = calib[\"P_rect_03\"]  # Right camera projection matrix (3x4)\n",
    "\n",
    "print(\"P_rect_02:\\n\", P2)\n",
    "print(\"\\nP_rect_03:\\n\", P3)\n",
    "\n",
    "# Intrinsics from left camera\n",
    "fx = P2[0, 0]\n",
    "fy = P2[1, 1]\n",
    "cx = P2[0, 2]\n",
    "cy = P2[1, 2]\n",
    "\n",
    "# Baseline: for rectified pair, P3[0,3] = -fx * B (left Tx assumed 0)\n",
    "baseline = (P2[0,3]-P3[0, 3]) / fx\n",
    "\n",
    "print(f\"\\nfx={fx:.3f}, fy={fy:.3f}, cx={cx:.3f}, cy={cy:.3f}, baseline={baseline:.4f} m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c7f7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: list images and helper to load frames\n",
    "\n",
    "def sorted_image_paths(img_dir):\n",
    "    files = [f for f in os.listdir(img_dir) if f.lower().endswith(\".png\")]\n",
    "    files = sorted(files)\n",
    "    return [os.path.join(img_dir, f) for f in files]\n",
    "\n",
    "left_paths  = sorted_image_paths(LEFT_DIR)\n",
    "right_paths = sorted_image_paths(RIGHT_DIR)\n",
    "\n",
    "num_frames = min(len(left_paths), len(right_paths))\n",
    "print(f\"Found {num_frames} frames in seq_03\")\n",
    "\n",
    "def load_frame(frame_idx):\n",
    "    if frame_idx < 0 or frame_idx >= num_frames:\n",
    "        raise IndexError(f\"Frame {frame_idx} out of range [0, {num_frames-1}]\")\n",
    "    left  = cv2.imread(left_paths[frame_idx])\n",
    "    right = cv2.imread(right_paths[frame_idx])\n",
    "    return left, right\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b1816f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: create stereo matcher for disparity\n",
    "\n",
    "# Parameters can be tuned\n",
    "window_size = 5\n",
    "min_disp = 0\n",
    "num_disp = 128  # must be divisible by 16\n",
    "\n",
    "stereo = cv2.StereoSGBM_create(\n",
    "    minDisparity=min_disp,\n",
    "    numDisparities=num_disp,\n",
    "    blockSize=7,\n",
    "    P1=8 * 3 * window_size**2,\n",
    "    P2=32 * 3 * window_size**2,\n",
    "    disp12MaxDiff=1,\n",
    "    uniquenessRatio=10,\n",
    "    speckleWindowSize=100,\n",
    "    speckleRange=32\n",
    ")\n",
    "\n",
    "def compute_disparity(left_bgr, right_bgr):\n",
    "    left_gray  = cv2.cvtColor(left_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    right_gray = cv2.cvtColor(right_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    disp = stereo.compute(left_gray, right_gray).astype(np.float32) / 16.0\n",
    "    # Invalid disparities are usually <= 0\n",
    "    return disp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b754216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: load YOLO detector (pretrained on COCO)\n",
    "\n",
    "# You can switch to a bigger model if GPU is available, e.g. \"yolov8m.pt\"\n",
    "yolo_model = YOLO(\"yolov8m.pt\")\n",
    "\n",
    "# Map COCO classes to your desired classes\n",
    "COCO_TO_OUR = {\n",
    "    \"person\": \"Pedestrian\",\n",
    "    \"bicycle\": \"Bike\",\n",
    "    \"car\": \"Car\",\n",
    "}\n",
    "\n",
    "def detect_objects(left_bgr, conf_thresh=0.25):\n",
    "    \"\"\"\n",
    "    Run YOLO on left image, return list of detections:\n",
    "    each detection is dict with keys:\n",
    "    'type', 'bbox', 'score'\n",
    "    bbox = [x1, y1, x2, y2] in pixel coordinates.\n",
    "    \"\"\"\n",
    "    results = yolo_model(left_bgr, imgsz=960, verbose=False)[0]\n",
    "\n",
    "    detections = []\n",
    "    for box in results.boxes:\n",
    "        cls_id = int(box.cls[0].item())\n",
    "        conf   = float(box.conf[0].item())\n",
    "        if conf < conf_thresh:\n",
    "            continue\n",
    "        x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
    "        coco_name = results.names[cls_id]\n",
    "        if coco_name not in COCO_TO_OUR:\n",
    "            continue\n",
    "\n",
    "        our_type = COCO_TO_OUR[coco_name]\n",
    "        detections.append({\n",
    "            \"type\": our_type,\n",
    "            \"bbox\": [x1, y1, x2, y2],\n",
    "            \"score\": conf\n",
    "        })\n",
    "    return detections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5a4040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: 3D estimation helpers\n",
    "\n",
    "def estimate_box_3d(bbox, disparity_map, fx, fy, cx, cy, baseline, min_valid_pixels=50):\n",
    "    \"\"\"\n",
    "    Given bbox [x1, y1, x2, y2] and disparity map, estimate:\n",
    "    X, Y, Z (m) and width, height (m) of object in camera coordinates.\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    x1_i, y1_i = int(max(0, x1)), int(max(0, y1))\n",
    "    x2_i, y2_i = int(min(disparity_map.shape[1]-1, x2)), int(min(disparity_map.shape[0]-1, y2))\n",
    "\n",
    "    if x2_i <= x1_i or y2_i <= y1_i:\n",
    "        return None  # invalid box\n",
    "\n",
    "    disp_crop = disparity_map[y1_i:y2_i, x1_i:x2_i]\n",
    "    valid_mask = disp_crop > 0.5  # threshold to filter garbage disparities\n",
    "\n",
    "    if valid_mask.sum() < min_valid_pixels:\n",
    "        return None  # too few valid disparity pixels\n",
    "\n",
    "    disp_valid = disp_crop[valid_mask]\n",
    "    # median_disp = np.median(disp_valid)\n",
    "    median_disp = np.percentile(disp_valid, 80)\n",
    "\n",
    "    if median_disp <= 0:\n",
    "        return None\n",
    "\n",
    "    # Depth\n",
    "    Z = fx * baseline / median_disp  # meters\n",
    "\n",
    "    # Box center in pixels\n",
    "    u = 0.5 * (x1 + x2)\n",
    "    v = 0.5 * (y1 + y2)\n",
    "\n",
    "    # Back-project to 3D\n",
    "    X = (u - cx) * Z / fx\n",
    "    Y = (v - cy) * Z / fy\n",
    "\n",
    "    # Approximate dimensions from pixel size at that depth\n",
    "    w_px = x2 - x1\n",
    "    h_px = y2 - y1\n",
    "\n",
    "    width_m  = w_px * Z / fx\n",
    "    height_m = h_px * Z / fy\n",
    "\n",
    "    return {\n",
    "        \"X\": float(X),\n",
    "        \"Y\": float(Y),\n",
    "        \"Z\": float(Z),\n",
    "        \"width_m\": float(width_m),\n",
    "        \"height_m\": float(height_m),\n",
    "        \"median_disp\": float(median_disp),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3862d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: drawing function\n",
    "\n",
    "def draw_detections(left_bgr, detections_with_3d):\n",
    "    \"\"\"\n",
    "    detections_with_3d: list of dicts with keys:\n",
    "      'type', 'bbox', 'score', 'X','Y','Z','width_m','height_m' (some may be None if 3D failed)\n",
    "    \"\"\"\n",
    "    img = left_bgr.copy()\n",
    "\n",
    "    for det in detections_with_3d:\n",
    "        x1, y1, x2, y2 = det[\"bbox\"]\n",
    "        obj_type = det[\"type\"]\n",
    "        score    = det[\"score\"]\n",
    "        pt1 = (int(x1), int(y1))\n",
    "        pt2 = (int(x2), int(y2))\n",
    "\n",
    "        color = (0, 255, 0)  # you can vary by class if you like\n",
    "        cv2.rectangle(img, pt1, pt2, color, 2)\n",
    "\n",
    "        # Prepare text lines\n",
    "        line1 = f\"{obj_type} ({score:.2f})\"\n",
    "        if det.get(\"Z\") is not None:\n",
    "            X, Y, Z = det[\"X\"], det[\"Y\"], det[\"Z\"]\n",
    "            Wm, Hm  = det[\"width_m\"], det[\"height_m\"]\n",
    "            line2 = f\"Z={Z:.1f}m, X={X:.1f}, Y={Y:.1f}\"\n",
    "            line3 = f\"H={Hm:.1f}m, W={Wm:.1f}m\"\n",
    "        else:\n",
    "            line2 = \"3D: n/a\"\n",
    "            line3 = \"\"\n",
    "\n",
    "        # Place text above/inside box\n",
    "        y0 = max(int(y1) - 30, 0)\n",
    "        # cv2.putText(img, line1, (int(x1), y0),cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2, cv2.LINE_AA)\n",
    "\n",
    "        y1_txt = max(int(y1) - 12, 0)\n",
    "        cv2.putText(img, line2, (int(x1), y1_txt),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2, cv2.LINE_AA)\n",
    "\n",
    "        if line3:\n",
    "            y2_txt = min(int(y2) + 15, img.shape[0]-5)\n",
    "            cv2.putText(img, line3, (int(x1), y2_txt),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2, cv2.LINE_AA)\n",
    "\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f53ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: sample a few frames and visualize detections + 3D info\n",
    "\n",
    "sample_indices = [0, num_frames // 4, num_frames // 2]  # pick 3 frames across sequence\n",
    "sample_indices = [idx for idx in sample_indices if 0 <= idx < num_frames]\n",
    "\n",
    "plt.figure(figsize=(15, 5 * len(sample_indices)))\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    left, right = load_frame(idx)\n",
    "    disparity = compute_disparity(left, right)\n",
    "\n",
    "    # Detect objects\n",
    "    dets = detect_objects(left, conf_thresh=0.4)\n",
    "\n",
    "    dets_with_3d = []\n",
    "    for det in dets:\n",
    "        info_3d = estimate_box_3d(det[\"bbox\"], disparity, fx, fy, cx, cy, baseline)\n",
    "        if info_3d is not None:\n",
    "            det_with_3d = {**det, **info_3d}\n",
    "        else:\n",
    "            det_with_3d = {**det, \"X\": None, \"Y\": None, \"Z\": None,\n",
    "                           \"width_m\": None, \"height_m\": None}\n",
    "        dets_with_3d.append(det_with_3d)\n",
    "\n",
    "    img_annotated = draw_detections(left, dets_with_3d)\n",
    "    img_rgb = cv2.cvtColor(img_annotated, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    plt.subplot(len(sample_indices), 1, i + 1)\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.title(f\"Frame {idx}: {len(dets_with_3d)} detections\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79d9515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: process entire sequence and save annotated video\n",
    "\n",
    "output_video_path = os.path.join(\"seq_03_annotated_yolo_3d.mp4\")\n",
    "fps = 10  # adjust to taste\n",
    "\n",
    "# Get frame size from first frame\n",
    "first_left, _ = load_frame(0)\n",
    "h, w = first_left.shape[:2]\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "writer = cv2.VideoWriter(output_video_path, fourcc, fps, (w, h))\n",
    "\n",
    "for idx in tqdm(range(num_frames), desc=\"Processing seq_03\"):\n",
    "    left, right = load_frame(idx)\n",
    "    disparity = compute_disparity(left, right)\n",
    "\n",
    "    dets = detect_objects(left, conf_thresh=0.4)\n",
    "\n",
    "    dets_with_3d = []\n",
    "    for det in dets:\n",
    "        info_3d = estimate_box_3d(det[\"bbox\"], disparity, fx, fy, cx, cy, baseline)\n",
    "        if info_3d is not None:\n",
    "            det_with_3d = {**det, **info_3d}\n",
    "        else:\n",
    "            det_with_3d = {**det, \"X\": None, \"Y\": None, \"Z\": None,\n",
    "                           \"width_m\": None, \"height_m\": None}\n",
    "        dets_with_3d.append(det_with_3d)\n",
    "        # write the detection info to a labels.txt file\n",
    "        line = f'{idx} {det_with_3d[\"type\"]} {det_with_3d[\"bbox\"][0]} {det_with_3d[\"bbox\"][1]} {det_with_3d[\"bbox\"][2]} {det_with_3d[\"bbox\"][3]} {det_with_3d[\"score\"]} {det_with_3d[\"X\"]} {det_with_3d[\"Y\"]} {det_with_3d[\"Z\"]} {det_with_3d[\"width_m\"]} {det_with_3d[\"height_m\"]}'\n",
    "        with open(\"labels.txt\", \"a\") as f:\n",
    "            f.write(f\"{line}\\n\")\n",
    "\n",
    "    frame_annotated = draw_detections(left, dets_with_3d)\n",
    "    writer.write(frame_annotated)\n",
    "\n",
    "writer.release()\n",
    "print(\"Saved annotated video to:\", output_video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1916df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: 3D Kalman filter (Constant Acceleration with Coherent Noise)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class KalmanFilter3D:\n",
    "    def __init__(self, dt=0.1, process_noise_jerk=5.0, meas_noise_pos=1.0):\n",
    "        \"\"\"\n",
    "        Constant-Acceleration Kalman filter in 3D with Physically Derived Noise.\n",
    "        \n",
    "        State: [x, y, z, vx, vy, vz, ax, ay, az]^T  (Dim: 9)\n",
    "        Measurement: [x, y, z]                      (Dim: 3)\n",
    "        \n",
    "        Args:\n",
    "            dt: Time step\n",
    "            process_noise_jerk: Variance of the 'Jerk' (change in accel). \n",
    "                                Higher = allows faster turns/changes.\n",
    "                                Lower = smoother path, more resistant to noise.\n",
    "            meas_noise_pos: Variance of the detection sensor (Lidar/Camera).\n",
    "        \"\"\"\n",
    "        self.dt = dt\n",
    "        self.dim = 9\n",
    "        \n",
    "        # --- 1. State Transition Matrix (F) ---\n",
    "        # Models the physics: p = p + vt + 0.5at^2\n",
    "        self.F = np.eye(self.dim)\n",
    "        \n",
    "        # Position updates\n",
    "        self.F[0, 3] = dt; self.F[1, 4] = dt; self.F[2, 5] = dt       # p += v*dt\n",
    "        self.F[0, 6] = 0.5*dt**2; self.F[1, 7] = 0.5*dt**2; self.F[2, 8] = 0.5*dt**2 # p += 0.5*a*dt^2\n",
    "        \n",
    "        # Velocity updates\n",
    "        self.F[3, 6] = dt; self.F[4, 7] = dt; self.F[5, 8] = dt       # v += a*dt\n",
    "\n",
    "        # --- 2. Measurement Matrix (H) ---\n",
    "        # We only measure Position [x, y, z]\n",
    "        self.H = np.zeros((3, self.dim))\n",
    "        self.H[0, 0] = 1.0\n",
    "        self.H[1, 1] = 1.0\n",
    "        self.H[2, 2] = 1.0\n",
    "\n",
    "        # --- 3. Process Noise Matrix (Q) ---\n",
    "        # We assume the system noise is a random \"Jerk\" (change in acceleration).\n",
    "        # We calculate how that jerk translates to Pos, Vel, and Accel errors over time dt.\n",
    "        # This creates a \"dense\" Q matrix that correlates the state variables.\n",
    "        \n",
    "        self.Q = np.zeros((self.dim, self.dim))\n",
    "        \n",
    "        # Coefficients for the Q matrix block derived from integral of white noise jerk\n",
    "        # (See Bar-Shalom \"Estimation with Applications to Tracking\" for derivation)\n",
    "        q_pos_pos = (dt**5) / 20.0\n",
    "        q_pos_vel = (dt**4) / 8.0\n",
    "        q_pos_acc = (dt**3) / 6.0\n",
    "        q_vel_vel = (dt**3) / 3.0\n",
    "        q_vel_acc = (dt**2) / 2.0\n",
    "        q_acc_acc = dt\n",
    "\n",
    "        # Apply these coefficients to X, Y, Z axes independently\n",
    "        # Axis Indices: X=[0,3,6], Y=[1,4,7], Z=[2,5,8]\n",
    "        for i in range(3): \n",
    "            p_idx = i       # Position index (0, 1, 2)\n",
    "            v_idx = i + 3   # Velocity index (3, 4, 5)\n",
    "            a_idx = i + 6   # Accel index    (6, 7, 8)\n",
    "            \n",
    "            # Position Row\n",
    "            self.Q[p_idx, p_idx] = q_pos_pos * process_noise_jerk\n",
    "            self.Q[p_idx, v_idx] = q_pos_vel * process_noise_jerk\n",
    "            self.Q[p_idx, a_idx] = q_pos_acc * process_noise_jerk\n",
    "            \n",
    "            # Velocity Row\n",
    "            self.Q[v_idx, p_idx] = q_pos_vel * process_noise_jerk\n",
    "            self.Q[v_idx, v_idx] = q_vel_vel * process_noise_jerk\n",
    "            self.Q[v_idx, a_idx] = q_vel_acc * process_noise_jerk\n",
    "            \n",
    "            # Acceleration Row\n",
    "            self.Q[a_idx, p_idx] = q_pos_acc * process_noise_jerk\n",
    "            self.Q[a_idx, v_idx] = q_vel_acc * process_noise_jerk\n",
    "            self.Q[a_idx, a_idx] = q_acc_acc * process_noise_jerk\n",
    "\n",
    "        # --- 4. Measurement Noise Matrix (R) ---\n",
    "        self.R = meas_noise_pos * np.eye(3)\n",
    "\n",
    "    def init_state(self, X, Y, Z):\n",
    "        \"\"\"Initialize state with zero vel/acc and high uncertainty\"\"\"\n",
    "        x = np.zeros((self.dim, 1))\n",
    "        x[0, 0] = X\n",
    "        x[1, 0] = Y\n",
    "        x[2, 0] = Z\n",
    "        \n",
    "        P = np.eye(self.dim)\n",
    "        P[0:3, 0:3] *= 10.0   # Position uncertainty\n",
    "        P[3:6, 3:6] *= 100.0  # Velocity uncertainty (we have no idea initially)\n",
    "        P[6:9, 6:9] *= 100.0  # Accel uncertainty (we have no idea initially)\n",
    "        \n",
    "        return x, P\n",
    "\n",
    "    def predict(self, x, P):\n",
    "        x = self.F @ x\n",
    "        P = self.F @ P @ self.F.T + self.Q\n",
    "        return x, P\n",
    "\n",
    "    def update(self, x, P, meas):\n",
    "        z = np.array(meas, dtype=float).reshape(3, 1)\n",
    "        y = z - self.H @ x                       # Innovation\n",
    "        S = self.H @ P @ self.H.T + self.R       # Innovation covariance\n",
    "        K = P @ self.H.T @ np.linalg.inv(S)      # Kalman gain\n",
    "        x = x + K @ y\n",
    "        P = (np.eye(self.dim) - K @ self.H) @ P\n",
    "        return x, P\n",
    "\n",
    "\n",
    "class Track:\n",
    "    _next_id = 0\n",
    "\n",
    "    def __init__(self, det, kf: KalmanFilter3D):\n",
    "        self.id = Track._next_id\n",
    "        Track._next_id += 1\n",
    "        self.kf = kf\n",
    "        \n",
    "        # Initialize KF state\n",
    "        X, Y, Z = det[\"X\"], det[\"Y\"], det[\"Z\"]\n",
    "        self.x, self.P = self.kf.init_state(X, Y, Z)\n",
    "\n",
    "        self.update_info(det)\n",
    "\n",
    "        self.age = 1\n",
    "        self.time_since_update = 0\n",
    "        self.hits = 1\n",
    "\n",
    "    def predict(self):\n",
    "        # Predict next state using physics model\n",
    "        self.x, self.P = self.kf.predict(self.x, self.P)\n",
    "        self.age += 1\n",
    "        self.time_since_update += 1\n",
    "\n",
    "    def update(self, det):\n",
    "        # Update state with new measurement\n",
    "        X, Y, Z = det[\"X\"], det[\"Y\"], det[\"Z\"]\n",
    "        self.x, self.P = self.kf.update(self.x, self.P, [X, Y, Z])\n",
    "        \n",
    "        self.update_info(det)\n",
    "        self.time_since_update = 0\n",
    "        self.hits += 1\n",
    "\n",
    "    def update_info(self, det):\n",
    "        \"\"\"Helper to update bbox and metadata\"\"\"\n",
    "        self.bbox = det[\"bbox\"]\n",
    "        self.type = det[\"type\"]\n",
    "        self.score = det[\"score\"]\n",
    "        x1, y1, x2, y2 = self.bbox\n",
    "        self.width_px  = float(x2 - x1)\n",
    "        self.height_px = float(y2 - y1)\n",
    "\n",
    "    # --- Getters for specific vectors ---\n",
    "    def get_position(self):\n",
    "        return self.x[0:3, 0].flatten() # Returns [x, y, z]\n",
    "\n",
    "    def get_velocity(self):\n",
    "        return self.x[3:6, 0].flatten() # Returns [vx, vy, vz]\n",
    "\n",
    "    def get_acceleration(self):\n",
    "        return self.x[6:9, 0].flatten() # Returns [ax, ay, az]\n",
    "\n",
    "    def get_state_str(self):\n",
    "        pos = self.get_position()\n",
    "        vel = self.get_velocity()\n",
    "        return f\"ID:{self.id} Pos:({pos[0]:.1f},{pos[1]:.1f}) Vel:({vel[0]:.1f},{vel[1]:.1f})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b447270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: associate detections with existing tracks using 3D distance\n",
    "\n",
    "def associate_detections_to_tracks(tracks, detections, max_dist=5.0):\n",
    "    \"\"\"\n",
    "    tracks:    list[Track]\n",
    "    detections: list[det dict with 'X','Y','Z']\n",
    "    Returns:\n",
    "        matches: list of (track_idx, det_idx)\n",
    "        unmatched_tracks: list of track indices\n",
    "        unmatched_detections: list of detection indices\n",
    "    \"\"\"\n",
    "    if len(tracks) == 0:\n",
    "        return [], [], list(range(len(detections)))\n",
    "    if len(detections) == 0:\n",
    "        return [], list(range(len(tracks))), []\n",
    "\n",
    "    cost = np.zeros((len(tracks), len(detections)), dtype=float)\n",
    "\n",
    "    for ti, tr in enumerate(tracks):\n",
    "        X_t, Y_t, Z_t = tr.get_position()\n",
    "        for di, det in enumerate(detections):\n",
    "            X_d, Y_d, Z_d = det[\"X\"], det[\"Y\"], det[\"Z\"]\n",
    "            if X_d is None or Y_d is None or Z_d is None:\n",
    "                cost[ti, di] = 1e6  # basically \"infinite\"\n",
    "            else:\n",
    "                # dist = np.linalg.norm([X_t - X_d, Y_t - Y_d, Z_t - Z_d])\n",
    "                dist = np.sqrt((X_t - X_d)**2 + (Y_t - Y_d)**2 + 0.1 * (Z_t - Z_d)**2)\n",
    "                cost[ti, di] = dist\n",
    "\n",
    "    row_ind, col_ind = linear_sum_assignment(cost)\n",
    "\n",
    "    matches = []\n",
    "    unmatched_tracks = list(range(len(tracks)))\n",
    "    unmatched_detections = list(range(len(detections)))\n",
    "\n",
    "    for r, c in zip(row_ind, col_ind):\n",
    "        if cost[r, c] > max_dist:\n",
    "            continue  # too far, don't match\n",
    "        matches.append((r, c))\n",
    "        if r in unmatched_tracks:\n",
    "            unmatched_tracks.remove(r)\n",
    "        if c in unmatched_detections:\n",
    "            unmatched_detections.remove(c)\n",
    "\n",
    "    return matches, unmatched_tracks, unmatched_detections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7859b901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: draw tracks (with IDs, 3D positions, and occlusion handling)\n",
    "\n",
    "def draw_tracks(left_bgr, tracks, fx, fy, cx, cy):\n",
    "    \"\"\"\n",
    "    Draws all active tracks on the image.\n",
    "    For tracks with time_since_update > 0, we draw predicted-only positions (yellow).\n",
    "    For updated tracks, we draw in green.\n",
    "    \"\"\"\n",
    "    img = left_bgr.copy()\n",
    "    H, W = img.shape[:2]\n",
    "\n",
    "    for tr in tracks:\n",
    "        X, Y, Z = tr.get_position()\n",
    "\n",
    "        # If depth goes crazy, skip drawing\n",
    "        if Z <= 0.5 or Z > 200.0:\n",
    "            continue\n",
    "\n",
    "        # Project 3D center -> image coordinates\n",
    "        u = fx * X / Z + cx\n",
    "        v = fy * Y / Z + cy\n",
    "\n",
    "        # Use stored box size (in pixels) to build bbox around (u, v)\n",
    "        w = tr.width_px\n",
    "        h = tr.height_px\n",
    "\n",
    "        x1 = int(u - w / 2)\n",
    "        y1 = int(v - h / 2)\n",
    "        x2 = int(u + w / 2)\n",
    "        y2 = int(v + h / 2)\n",
    "\n",
    "        # Clamp to image bounds\n",
    "        x1 = max(0, min(W - 1, x1))\n",
    "        x2 = max(0, min(W - 1, x2))\n",
    "        y1 = max(0, min(H - 1, y1))\n",
    "        y2 = max(0, min(H - 1, y2))\n",
    "\n",
    "        # Color: green if updated this frame, yellow if only predicted\n",
    "        if tr.time_since_update == 0:\n",
    "            color = (0, 255, 0)   # updated\n",
    "            status = \"\"\n",
    "        else:\n",
    "            color = (0, 255, 255) # predicted\n",
    "            status = \" (pred)\"\n",
    "\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
    "\n",
    "        line1 = f\"ID {tr.id} {tr.type}{status}\"\n",
    "        line2 = f\"Z={Z:.1f}m, X={X:.1f}, Y={Y:.1f}\"\n",
    "\n",
    "        y0 = max(y1 - 25, 0)\n",
    "        y1_txt = max(y1 - 7, 0)\n",
    "\n",
    "        cv2.putText(img, line1, (x1, y0),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2, cv2.LINE_AA)\n",
    "        cv2.putText(img, line2, (x1, y1_txt),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2, cv2.LINE_AA)\n",
    "\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb213d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: run tracker on a few consecutive frames for visualization\n",
    "\n",
    "# Make sure fps exists; if not, define it (used for dt)\n",
    "try:\n",
    "    fps\n",
    "except NameError:\n",
    "    fps = 10.0\n",
    "\n",
    "tracks = []\n",
    "Track._next_id = 0\n",
    "\n",
    "# kf_3d = KalmanFilter3D(dt=1.0 / fps, process_var=5.0, meas_var=1.0)\n",
    "kf_3d = KalmanFilter3D(dt=1.0/fps)\n",
    "max_age = 50        # frames to keep \"dead\" tracks alive (3s if fps=10)\n",
    "max_dist = 40.0     # max 3D distance in meters for association\n",
    "min_3d_conf_pixels = 50  # you can keep this, or slightly lower if needed\n",
    "\n",
    "start_idx = 0\n",
    "end_idx = min(start_idx + 20, num_frames)  # small window\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 4 * (end_idx - start_idx)//5 + 5))\n",
    "\n",
    "plot_row = 0\n",
    "\n",
    "for frame_idx in range(start_idx, end_idx):\n",
    "    left, right = load_frame(frame_idx)\n",
    "    disparity = compute_disparity(left, right)\n",
    "\n",
    "    # Step 1: detect objects in this frame\n",
    "    dets = detect_objects(left, conf_thresh=0.4)\n",
    "\n",
    "    # Step 2: get 3D for each detection (skip those without valid 3D)\n",
    "    dets3d = []\n",
    "    for det in dets:\n",
    "        info_3d = estimate_box_3d(det[\"bbox\"], disparity, fx, fy, cx, cy, baseline,\n",
    "                                  min_valid_pixels=min_3d_conf_pixels)\n",
    "        if info_3d is not None:\n",
    "            dets3d.append({**det, **info_3d})\n",
    "\n",
    "    # Step 3: predict all existing tracks\n",
    "    for tr in tracks:\n",
    "        tr.predict()\n",
    "\n",
    "    # Step 4: associate detections to tracks\n",
    "    matches, unmatched_tracks, unmatched_dets = associate_detections_to_tracks(\n",
    "        tracks, dets3d, max_dist=max_dist\n",
    "    )\n",
    "\n",
    "    # Step 5: update matched tracks\n",
    "    for t_idx, d_idx in matches:\n",
    "        tracks[t_idx].update(dets3d[d_idx])\n",
    "\n",
    "    # Step 6: create new tracks for unmatched detections\n",
    "    for d_idx in unmatched_dets:\n",
    "        tr = Track(dets3d[d_idx], kf_3d)\n",
    "        tracks.append(tr)\n",
    "\n",
    "    # Step 7: remove dead tracks\n",
    "    new_tracks = []\n",
    "    for tr in tracks:\n",
    "        if tr.time_since_update <= max_age:\n",
    "            new_tracks.append(tr)\n",
    "    tracks = new_tracks\n",
    "\n",
    "    # Visualize every 5th frame\n",
    "    if frame_idx % 5 == 0:\n",
    "        plot_row += 1\n",
    "        img_tr = draw_tracks(left, tracks, fx, fy, cx, cy)\n",
    "        img_tr_rgb = cv2.cvtColor(img_tr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        plt.subplot((end_idx - start_idx) // 5 + 1, 1, plot_row)\n",
    "        plt.imshow(img_tr_rgb)\n",
    "        plt.title(f\"Frame {frame_idx} with {len(tracks)} active tracks\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2355debf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: full seq_03 tracking video with IDs and occlusion prediction\n",
    "\n",
    "output_video_path_tracks = os.path.join(\"seq_03_tracking_3d.mp4\")\n",
    "fps = 10  # same as before, or adjust\n",
    "\n",
    "first_left, _ = load_frame(0)\n",
    "h, w = first_left.shape[:2]\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "writer = cv2.VideoWriter(output_video_path_tracks, fourcc, fps, (w, h))\n",
    "\n",
    "# Re-init tracker state for full run\n",
    "# kf_3d = KalmanFilter3D(dt=1.0 / fps, process_var=5.0, meas_var=1.0)\n",
    "kf_3d = KalmanFilter3D(dt=1.0/fps)\n",
    "max_age = 50        # frames to keep \"dead\" tracks alive (3s if fps=10)\n",
    "max_dist = 40.0     # max 3D distance in meters for association\n",
    "min_3d_conf_pixels = 50  # you can keep this, or slightly lower if needed\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for frame_idx in tqdm(range(num_frames), desc=\"Tracking seq_03\"):\n",
    "    left, right = load_frame(frame_idx)\n",
    "    disparity = compute_disparity(left, right)\n",
    "\n",
    "    dets = detect_objects(left, conf_thresh=0.4)\n",
    "\n",
    "    dets3d = []\n",
    "    for det in dets:\n",
    "        info_3d = estimate_box_3d(det[\"bbox\"], disparity, fx, fy, cx, cy, baseline,\n",
    "                                  min_valid_pixels=min_3d_conf_pixels)\n",
    "        if info_3d is not None:\n",
    "            dets3d.append({**det, **info_3d})\n",
    "\n",
    "    # Predict step\n",
    "    for tr in tracks:\n",
    "        tr.predict()\n",
    "\n",
    "    # Associate\n",
    "    matches, unmatched_tracks, unmatched_dets = associate_detections_to_tracks(\n",
    "        tracks, dets3d, max_dist=max_dist\n",
    "    )\n",
    "\n",
    "    # Update matched\n",
    "    for t_idx, d_idx in matches:\n",
    "        tracks[t_idx].update(dets3d[d_idx])\n",
    "\n",
    "    # New tracks\n",
    "    for d_idx in unmatched_dets:\n",
    "        tracks.append(Track(dets3d[d_idx], kf_3d))\n",
    "\n",
    "    # Remove stale\n",
    "    new_tracks = []\n",
    "    for tr in tracks:\n",
    "        if tr.time_since_update <= max_age:\n",
    "            new_tracks.append(tr)\n",
    "    tracks = new_tracks\n",
    "\n",
    "    # Draw\n",
    "    frame_annotated = draw_tracks(left, tracks, fx, fy, cx, cy)\n",
    "    writer.write(frame_annotated)\n",
    "\n",
    "writer.release()\n",
    "print(\"Saved tracking video to:\", output_video_path_tracks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251d6fd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
